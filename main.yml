---
# Main playbook: RHEL Server Build Automation

# ----------------------------
# 1️⃣ Root Password Change
# ----------------------------
- name: Change root password
  hosts: all
  become: yes
  gather_facts: no
  tags:
    - root
  tasks:
    - name: Set root password
      user:
        name: root
        password: "{{ '$NDK~R3d^H@t' | password_hash('sha512') }}"
      register: root_pw_result
      ignore_errors: yes

    - name: Log root password change
      import_tasks: log_task_status.yml
      vars:
        task_result: "{{ root_pw_result }}"
        task_name: "Root password change"

# ----------------------------
# 2️⃣ Hostname Change
# ----------------------------
- name: Change system hostname
  hosts: all
  become: yes
  gather_facts: no
  tags:
    - hostname
  tasks:
    - block:
        - name: Set system hostname using inventory name
          hostname:
            name: "{{ inventory_hostname }}"
          register: hostname_result

      rescue:
        - set_fact:
            hostname_result:
              failed: true
              msg: "Hostname change failed"

      always:
        - name: Log hostname change
          import_tasks: log_task_status.yml
          vars:
            task_result: "{{ hostname_result }}"
            task_name: "Change Hostname"


# ----------------------------
# 3️⃣ resolv.conf Management
# ----------------------------
- name: Backup, update, and verify resolv.conf
  hosts: all
  become: yes
  gather_facts: yes
  tags:
    - resolv
  vars:
    date_suffix: "{{ ansible_date_time.day }}{{ ansible_date_time.month | upper }}{{ ansible_date_time.year }}"
  tasks:
    - name: Backup existing /etc/resolv.conf
      copy:
        src: /etc/resolv.conf
        dest: "/etc/resolv.conf_bkp_{{ date_suffix }}"
        owner: root
        group: root
        mode: '0644'
        backup: no
      ignore_errors: yes

    - name: Update /etc/resolv.conf
      copy:
        dest: /etc/resolv.conf
        content: |
          search sandisk.com corp.sandisk.com
          nameserver 10.86.1.1
          nameserver 10.86.2.1
        owner: root
        group: root
        mode: '0644'
      register: update_result
      ignore_errors: yes

    - name: Log resolv.conf update
      import_tasks: log_task_status.yml
      vars:
        task_result: "{{ update_result }}"
        task_name: "Update /etc/resolv.conf"

    - name: Make /etc/resolv.conf immutable
      command: chattr +i /etc/resolv.conf
      ignore_errors: yes

    - name: Verify /etc/resolv.conf is immutable
      command: lsattr /etc/resolv.conf
      register: verify_result
      ignore_errors: yes

    - debug:
        msg: "{{ verify_result.stdout }}"

# ----------------------------
# 4️⃣ Subscription Registration
# ----------------------------
- name: Subscription registration
  hosts: all
  gather_facts: yes
  tags:
    - subscription
  vars:
    site_map:
      uls: uls
      ibp: ibp
      iky: iky
      jtx: jtx
      use: use
      mpp: jtx
      ibn: ibp       # ibn servers → use ibp subscription
  tasks:
    - name: Get OS major version
      set_fact:
        os_major: "{{ ansible_distribution_major_version }}"
      tags:
        - subscription

    - name: Determine environment letter
      set_fact:
        env_letter: "{{ ansible_hostname.split('-')[1][-1] }}"
      tags:
        - subscription

    - name: Extract site prefix
      set_fact:
        raw_prefix: "{{ ansible_hostname.split('-')[0] }}"
        site_prefix: "{{ site_map[ansible_hostname.split('-')[0]] | default(ansible_hostname.split('-')[0]) }}"
      tags:
        - subscription

    - name: Show detected values
      debug:
        msg:
          - "Raw Site Prefix: {{ raw_prefix }}"
          - "Mapped Site Prefix: {{ site_prefix }}"
          - "Environment Letter: {{ env_letter }}"
          - "OS Major: {{ os_major }}"
      tags:
        - subscription

    - name: Include site-specific registration tasks
      include_tasks: "sites/{{ site_prefix }}.yml"
      register: subscription_result
      ignore_errors: yes
      tags:
        - subscription

    - name: Log subscription status
      import_tasks: log_task_status.yml
      vars:
        task_result: "{{ subscription_result | default({}) }}"
        task_name: "RHEL{{ os_major }} Subscription ({{ raw_prefix }} → {{ site_prefix }})"

# ----------------------------
# 5️⃣ Domain Join & SSSD Configuration
# ----------------------------


- name: Join Linux server to Active Directory domain
  hosts: all
  become: true
  tags:
    - Domain_join

  vars_files:
    - vars/domain_credentials.yml   # contains domain_user, domain_password, domain_name, ou_map
    #- vars/sssd_vars.yml            # contains sssd_groups list

  tasks:
    ##################################################################
    # Install required packages
    ##################################################################
    - name: Install required packages for realm join
      yum:
        name:
          - realmd
          - sssd
          - sssd-tools
          - adcli
          - krb5-workstation
          - oddjob
          - oddjob-mkhomedir
          - samba-common
          - samba-common-tools
          - authselect-compat
        state: present
      register: sssd_install

    ##################################################################
    # Detect environment
    ##################################################################
    - name: Detect short hostname
      set_fact:
        short_hostname: "{{ inventory_hostname.split('.')[0] }}"

    - name: Detect environment from hostname (6th letter)
      set_fact:
        env_letter: "{{ short_hostname[5] }}"
        domain_env: >-
          {% if short_hostname[5] == 'p' %}PROD
          {% elif short_hostname[5] == 't' %}TEST
          {% elif short_hostname[5] == 'd' %}DEV
          {% else %}UNKNOWN{% endif %}

    - name: Normalize environment name (trim spaces)
      set_fact:
        domain_env: "{{ domain_env | trim }}"

    - name: Fail if environment could not be determined
      fail:
        msg: "Unable to determine environment from hostname: {{ short_hostname }}"
      when: domain_env == "UNKNOWN"

    - name: Set the OU based on detected environment
      set_fact:
        domain_ou: "{{ ou_map[domain_env] }}"
      ignore_errors: yes

    - name: Show detected environment and OU
      debug:
        msg: "Hostname={{ short_hostname }}, Environment={{ domain_env }}, OU={{ domain_ou }}"
      ignore_errors: yes

    ##################################################################
    # Domain Join
    ##################################################################
    - name: Join system to AD domain
      #command: >
#        realm join --user={{ domain_user }} {{ domain_name }}--computer-ou="{{ domain_ou }}"
      command: >
        realm join {{ domain_name }}
        --user={{ domain_user }}
        --computer-ou="{{ domain_ou }}"
      args:
        stdin: "{{ domain_password }}"
      register: ansible_output
      ignore_errors: yes
      tags:
        - Domain_join

    ##################################################################
    # Post-Join: Manage sssd.conf
    ##################################################################
    - name: Check if /etc/sssd/sssd.conf exists
      stat:
        path: /etc/sssd/sssd.conf
      register: sssd_conf
      tags:
        - Domain_join


    - name: Backup existing sssd.conf with date
      copy:
        src: /etc/sssd/sssd.conf
        dest: "/etc/sssd/sssd.conf_bak_{{ lookup('pipe', 'date +%d%b%Y') }}"
        remote_src: yes
      when: sssd_conf.stat.exists
      tags:
        - Domain_join

    - name: Deploy new sssd.conf
      copy:
        dest: /etc/sssd/sssd.conf
        content: |
          [sssd]
          domains = corp.sandisk.com
          config_file_version = 2
          services = nss, pam
          timeout = 150

          [nss]
          homedir_substring = /home
          timeout = 150

          [domain/corp.sandisk.com]
          ad_domain = corp.sandisk.com
          #ad_site = corp.sandisk.com
          timeout = 150
          krb5_realm = corp.sandisk.com
          realmd_tags = manages-system joined-with-samba
          cache_credentials = True
          id_provider = ad
          krb5_store_password_if_offline = True
          krb5_auth_timeout = 30
          default_shell = /bin/bash
          ldap_id_mapping = True
          use_fully_qualified_names = False
          fallback_homedir = /home/%u
          access_provider = ad
          simple_allow_users =
          simple_allow_groups = IT-Infra-Linux-Support-Flash, IT-Infra-Linux-Support{% if sssd_groups is defined and sssd_groups|length > 0 %}, {{ sssd_groups | join(', ') }}{% endif %}

          ad_gpo_ignore_unreadable = True
          #ldap_user_principal = nosuchattr
          subdomain_inherit = ignore_group_members, ldap_purge_cache_timeout
          ignore_group_members = True
          ldap_purge_cache_timeout = 0
          ad_enable_gc = False
          override_homedir = /home/%u
          ldap_use_tokengroups = false
          dns_resolver_timeout = 60
          dyndns_update = false
          ad_gpo_access_control = disabled

          [pam]
          timeout = 150

          [pac]
          timeout = 150
        owner: root
        group: root
        mode: '0600'
      register: sssd_config
      tags:
        - Domain_join

    ##################################################################
    # Clean SSSD cache and restart
    ##################################################################
    - name: Stop sssd
      systemd:
        name: sssd
        state: stopped

    - name: Clear sssd cache and logs
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /var/log/sssd
        - /var/lib/sss/db
        - /var/lib/sss/mc

    - name: Recreate directories for SSSD
      file:
        path: "{{ item.path }}"
        state: directory
        owner: "{{ item.owner }}"
        group: "{{ item.group }}"
        mode: "{{ item.mode }}"
      loop:
        - { path: '/var/log/sssd', owner: 'sssd', group: 'sssd', mode: '0755' }
        - { path: '/var/lib/sss/db', owner: 'sssd', group: 'sssd', mode: '0700' }
        - { path: '/var/lib/sss/mc', owner: 'sssd', group: 'sssd', mode: '0700' }

    - name: Start sssd
      systemd:
        name: sssd
        state: started
        enabled: yes
      ignore_errors: yes
      tags:
        - Domain_join

    ##################################################################
    # Configure sudoers (hardcoded only)
    ##################################################################
    - name: Configure sudo access for AD groups
      copy:
        dest: /etc/sudoers.d/sssd_admin
        content: |
          %IT-Infra-Linux-Support-Flash ALL=(ALL:ALL) NOPASSWD: ALL
          %IT-Infra-Linux-Support ALL=(ALL:ALL) NOPASSWD: ALL
        owner: root
        group: root
        mode: '0440'
        tags:
         - Domain_join

    ##################################################################
    # Final Logging
    ##################################################################
    - name: Log domain join task status
      import_tasks: log_task_status.yml
      vars:
        task_result: "{{ ansible_output }}"
        task_name: "Join system to AD domain"

    - name: Log SSSD install status
      import_tasks: log_task_status.yml
      vars:
        task_result: "{{ sssd_install }}"
        task_name: "SSSD package installation"

    - name: Log SSSD config status
      import_tasks: log_task_status.yml
      vars:
        task_result: "{{ sssd_config }}"
        task_name: "SSSD configuration"

# ----------------------------
# Timezone Configuration
# ----------------------------

- name: Configure system timezone based on hostname
  hosts: all
  become: yes
  tags:
    - timezone
  vars:
    timezones:
      ULS: "America/Los_Angeles"
      USE: "America/Los_Angeles"
      USS: "America/Los_Angeles"
      USG: "America/Los_Angeles"
      UIM: "America/Los_Angeles"
      TBM: "Asia/Bangkok"
      TBB: "Asia/Bangkok"
      TPT: "Asia/Bangkok"
      IKY: "Asia/Jerusalem"
      CSJ: "Asia/Shanghai"
      CSS: "Asia/Shanghai"
      CSF: "Asia/Shanghai"
      IOI: "Asia/Jerusalem"
      IBP: "Asia/Kolkata"
      IBN: "Asia/Kolkata" ##added new manually 
      IBS: "Asia/Kolkata"
      IBV: "Asia/Kolkata"
      IBT: "Asia/Kolkata"
      KSG: "Asia/Seoul"
      MSK: "Asia/Kuala_Lumpur"
      MJP: "Asia/Kuala_Lumpur"
      MPP: "Asia/Kuala_Lumpur"
      MPL: "Asia/Kuala_Lumpur"
      MPS: "Asia/Kuala_Lumpur"
      PBT: "Asia/Manila"
      JAN: "Asia/Tokyo"
      JFK: "Asia/Tokyo"
      JOK: "Asia/Tokyo"
      JOM: "Asia/Tokyo"
      JTK: "Asia/Tokyo"
      JON: "Asia/Tokyo"
      JYU: "Asia/Tokyo"
      JOO: "Asia/Tokyo"
      JTY: "Asia/Tokyo"
      JYY: "Asia/Tokyo"
      IBH: "Asia/Kolkata"

  tasks:
    - name: Detect site code from hostname
      set_fact:
        site_code: "{{ item }}"
      when: inventory_hostname | upper is search(item)
      loop: "{{ timezones.keys() }}"

    - name: Fail if no site code detected
      set_fact:
        tz_result:
          failed: true
          msg: "No matching location code found in {{ inventory_hostname }}"

    - name: Set timezone
      community.general.timezone:
        name: "{{ timezones[site_code] }}"
      register: tz_result
      when: site_code is defined
      ignore_errors: true

    - name: Log timezone task
      import_tasks: log_task_status.yml
      vars:
        task_result: "{{ tz_result }}"
        task_name: "Timezone Change ({{ site_code | default('N/A') }} → {{ timezones[site_code] | default('Unknown') }})"

# ----------------------------
# 6️⃣ Chrony Install & Config
# ----------------------------
- name: Install and configure Chrony
  hosts: all
  become: yes
  tags:
    - chrony
  tasks:
    - block:
        - name: Ensure chrony is installed
          yum:
            name: chrony
            state: present
          ignore_errors: yes
          register: chrony_install

        - name: Enable/start chronyd
          systemd:
            name: chronyd
            state: started
            enabled: yes
          ignore_errors: yes
          register: chrony_service

        - name: Configure chrony.conf
          copy:
            dest: /etc/chrony.conf
            content: |
              # Internal NTP servers
              server 10.86.1.1 iburst
              server 10.86.2.1 iburst
              driftfile /var/lib/chrony/drift
              makestep 1.0 3
              rtcsync
          notify: Restart chronyd
          ignore_errors: yes
          register: chrony_config

      rescue:
        - debug:
            msg: "Chrony block failed"

      always:
        - import_tasks: log_task_status.yml
          vars:
            task_result: "{{ chrony_install | default({}) }}"
            task_name: "Chrony Installation"

        - import_tasks: log_task_status.yml
          vars:
            task_result: "{{ chrony_service | default({}) }}"
            task_name: "Chrony Service Start"

        - import_tasks: log_task_status.yml
          vars:
            task_result: "{{ chrony_config | default({}) }}"
            task_name: "Chrony Configuration"

  handlers:
    - name: Restart chronyd
      service:
        name: chronyd
        state: restarted
      ignore_errors: yes


#----------------------------
#Check -mk
# ----------------------------
- name: Install and configure Checkmk agent
  hosts: all
  become: yes
  tags:
    - checkmk
  tasks:
    - block:
        - name: Copy Checkmk agent package
          copy:
            src: Packages/check-mk-agent-2.3.0p30-1.noarch.rpm
            dest: /tmp/check-mk-agent.rpm
            owner: root
            group: root
            mode: '0644'

        - name: Install Checkmk agent
          dnf:
            name: /tmp/check-mk-agent.rpm
            state: present
            disable_gpg_check: yes

      rescue:
        - debug:
            msg: "Checkmk block failed"

      always:
        - import_tasks: log_task_status.yml
          vars:
            task_result: "{{ ansible_failed_task | default({}) }}"
            task_name: "Checkmk Agent Install"

# ----------------------------
# 8️⃣ Falcon Sensor
# ----------------------------
- name: Install and configure Falcon sensor
  hosts: all
  become: yes
  tags:
    - falcon

  tasks:
    - block:
        - name: Stop falcon-sensor if running
          systemd:
            name: falcon-sensor
            state: stopped
          ignore_errors: yes   # in case it's not installed yet

        - name: Remove old Falcon files
          file:
            path: "{{ item }}"
            state: absent
          loop:
            - /opt/CrowdStrike/Registry.bin
          notify: Restart falcon-sensor

        - name: Remove old Falcon C-* files
          shell: "rm -f /opt/CrowdStrike/C-*"
          args:
            warn: false
          ignore_errors: yes

        - name: Copy Falcon sensor package
          copy:
            src: Packages/falcon-sensor-7.28.0-18108.el9.x86_64.rpm
            dest: /tmp/falcon-sensor.rpm
            owner: root
            group: root
            mode: '0644'

        - name: Install Falcon sensor
          dnf:
            name: /tmp/falcon-sensor.rpm
            state: present
            disable_gpg_check: yes

        - name: Configure Falcon CID
          command: /opt/CrowdStrike/falconctl -s --cid=06C18613D2124D6CA8757655E830126E-83 -f

        - name: Start falcon-sensor service
          systemd:
            name: falcon-sensor
            state: started
            enabled: yes

        # Verification tasks
        - name: Verify falcon-sensor process is running
          shell: "ps -e | grep -e falcon-sensor"
          register: falcon_process
          changed_when: false
          failed_when: falcon_process.rc != 0

        - name: Verify Falcon package is installed
          shell: "rpm -qa | grep falcon"
          register: falcon_pkg
          changed_when: false
          failed_when: falcon_pkg.rc != 0

        - name: Verify Falcon ports are open
          shell: "netstat -tapn | grep falcon"
          register: falcon_netstat
          changed_when: false
          failed_when: falcon_netstat.rc != 0

      rescue:
        - debug:
            msg: "Falcon block failed"

      always:
        - import_tasks: log_task_status.yml
          vars:
            task_result: "{{ ansible_failed_task | default({}) }}"
            task_name: "Falcon Sensor Install"

  handlers:
    - name: Restart falcon-sensor
      systemd:
        name: falcon-sensor
        state: restarted


# ----------------------------
# 9️⃣ Team SSH Keys
# ----------------------------

- name: Add Team keys to authorized_keys
  hosts: all
  become: yes
  tags: 
    - Team_keys
  tasks:
    - block:
        - name: Ensure root .ssh directory exists
          file:
            path: /root/.ssh
            state: directory
            mode: '0700'
            owner: root
            group: root

        - name: Copy team keys to root authorized_keys
          copy:
            src: sandisk_teamkeys/SnDkTeam.keys
            dest: /root/.ssh/authorized_keys
            owner: root
            group: root
            mode: '0600'
          register: sshkeys_result

      rescue:
        - set_fact:
            sshkeys_result: { failed: true }

      always:
        - import_tasks: log_task_status.yml
          vars:
            task_result: "{{ sshkeys_result | default({}) }}"
            task_name: "Copy Team SSH keys"


# ----------------------------
# 10️⃣ SSHD Modification
# ----------------------------
- name: Modify SSHD configuration
  hosts: all
  become: yes

  tags: sshd_modify

  tasks:
    - name: Backup sshd_config
      copy:
        src: /etc/ssh/sshd_config
        dest: "/etc/ssh/sshd_config_bak_{{ lookup('pipe','date +%d%b%Y') }}"
        remote_src: yes
      ignore_errors: yes

    - name: Ensure PermitRootLogin yes
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#?PermitRootLogin'
        line: 'PermitRootLogin yes'
        insertafter: EOF
        backup: yes

    - name: Disable GSSAPI Authentication
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#?GSSAPIAuthentication'
        line: 'GSSAPIAuthentication no'
        insertafter: EOF

    - name: Enable GSSAPI Cleanup Credentials
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#?GSSAPICleanupCredentials'
        line: 'GSSAPICleanupCredentials yes'
        insertafter: EOF

    - name: Restart sshd to apply changes
      systemd:
        name: sshd
        state: restarted
      ignore_errors: yes


    

# ----------------------------
- name: Ensure Postfix is installed and restarted
  hosts: all
  become: yes
  tags:
    - postfix
  tasks:
    - name: Install Postfix
      package:
        name: postfix
        state: present
      ignore_errors: yes
      register: postfix_install

    - name: Restart Postfix service
      service:
        name: postfix
        state: restarted
        enabled: yes
      ignore_errors: yes
      register: postfix_restart

    - name: Log Postfix installation status
      import_tasks: log_task_status.yml
      vars:
        task_result: "{{ postfix_install }}"
        task_name: "Postfix Install"

    - name: Log Postfix restart status
      import_tasks: log_task_status.yml
      vars:
        task_result: "{{ postfix_restart }}"
        task_name: "Postfix Restart"

# ----------------------------
# 13 LVM (datavg) Creation
# ----------------------------

- name: LVM Setup - Create PV and VG (single disk)
  hosts: all
  become: true
  tags:
    - lvm

  vars_files:
    - vars/vg_name.yml     # loads vg_name

  tasks:

    - block:

        ####################################################################
        # 1. LIST ALL DISKS
        ####################################################################
        - name: Find all non-OS disks
          command: lsblk -d -n -o NAME
          register: disks_output

        ####################################################################
        # 2. SELECT FIRST DISK THAT IS NOT sda OR sr0
        ####################################################################
        - name: Pick valid candidate disk
          set_fact:
            candidate_list: "{{ disks_output.stdout_lines
                                | reject('match', '^sda$')
                                | reject('match', '^sr0$')
                                | list }}"
        
        - name: Set candidate_disk (first available)
          set_fact:
            candidate_disk: "/dev/{{ candidate_list[0] }}"
          when: candidate_list | length > 0

        ####################################################################
        # 3. IF NO DISK → SKIP CLEANLY
        ####################################################################
        - name: Skip LVM setup when no disk found
          when: candidate_list | length == 0
          block:
            - debug:
                msg: "❌ No available data disk found — skipping LVM setup."
            - set_fact:
                lvm_final_status:
                  skipped: true
                  failed: false
                  msg: "No available data disk found. LVM setup skipped."

        ####################################################################
        # 4. CONTINUE ONLY IF DISK EXISTS
        ####################################################################
        - name: Proceed only if disk exists
          when: candidate_list | length > 0
          block:

            - name: Check existing PVs
              command: pvs --noheadings -o pv_name
              register: existing_pvs
              changed_when: false

            - name: Create PV if missing
              command: pvcreate {{ candidate_disk }}
              register: pvcreate_result
              when: candidate_disk not in existing_pvs.stdout_lines
              ignore_errors: yes

            - name: Check if VG exists
              command: vgs {{ vg_name }} --noheadings
              register: vg_exists
              failed_when: false
              changed_when: false

            - name: Create VG if missing
              command: vgcreate {{ vg_name }} {{ candidate_disk }}
              register: vgcreate_result
              when: vg_exists.stdout == ""
              ignore_errors: yes

            - name: Set success result
              set_fact:
                lvm_final_status:
                  failed: "{{ (pvcreate_result is defined and pvcreate_result.failed|default(false)) 
                              or (vgcreate_result is defined and vgcreate_result.failed|default(false)) }}"
                  skipped: false
                  msg: >-
                    {% if vg_exists.stdout != "" %}
                      VG {{ vg_name }} already existed — no changes.
                    {% elif pvcreate_result is defined and pvcreate_result.failed|default(false) %}
                      PV creation failed.
                    {% elif vgcreate_result is defined and vgcreate_result.failed|default(false) %}
                      VG creation failed.
                    {% else %}
                      PV/VG setup completed successfully.
                    {% endif %}

      rescue:
        - set_fact:
            lvm_final_status:
              failed: true
              skipped: false
              msg: "Unexpected LVM failure occurred."

      always:
        - name: Log LVM setup status
          import_tasks: log_task_status.yml
          vars:
            task_name: "LVM Setup (PV + VG)"
            task_result: "{{ lvm_final_status }}"        

#########################################################################################
# #                                               Delete it   #################
# # ----------------------------
# # 13 LVM (datavg) Creation
# # ----------------------------

# #- name: LVM Setup - Create PV and VG (single disk)
#  # hosts: all
#   become: true
#   tags:
#     - lvm
#   tasks:
#     - block:
#         - name: Find all non-OS disks
#           command: lsblk -d -n -o NAME
#           register: disks_output

#         - name: Select candidate disk (exclude sda, sr0)
#           set_fact:
#             candidate_disk: "/dev/{{ item }}"
#           loop: "{{ disks_output.stdout_lines }}"
#           when:
#             - item != "sda"
#             - item != "sr0"
#           run_once: true

#         - name: Check if PV exists on candidate disk
#           command: pvs --noheadings -o pv_name
#           register: existing_pvs
#           changed_when: false

#         - name: Create PV if it does not exist
#           command: pvcreate {{ candidate_disk }}
#           register: pvcreate_result
#           when: candidate_disk not in existing_pvs.stdout
#           ignore_errors: yes

#         - name: Check if VG "datavg" exists
#           command: vgs VG_OPENV --noheadings
#           register: vg_exists
#           failed_when: false
#           changed_when: false

#         - name: Create VG "datavg" on candidate disk if not exists
#           command: vgcreate VG_OPENV {{ candidate_disk }}
#           register: vgcreate_result
#           when:
#             - candidate_disk is defined
#             - vg_exists.stdout == ""
#           ignore_errors: yes

#       rescue:
#         - set_fact:
#             lvm_result:
#               failed: true
#               msg: "LVM PV or VG creation failed"

#       always:
#         - name: Log LVM setup status
#           import_tasks: log_task_status.yml
#           vars:
#             task_name: "LVM Setup (PV + VG)"
#             task_result: "{{ vgcreate_result | default(pvcreate_result) | default(lvm_result | default({'failed': false})) }}"
###########################################################################################

## ----------------------------
## Common Utilities Install
## ----------------------------
- name: Install common utilities
  hosts: all
  become: yes
  tags:
    - common_utils
  tasks:
    - block:
        - name: Ensure common utilities are installed
          yum:
            name:
              - wget
              - sysstat
              - openssl
              - at
              - bzip2
              - git
              - htop
              - iproute
              - lsof
              - nfs-utils
              - pcp
              - rsync
              - screen
              - tcpdump
              - telnet
              - tmux
              - traceroute
              - unzip
              - s-nail
              - vim
              - zip
              - zsh
              - ksh
              - sos
            state: present

        - debug:
            msg: "=== Common utilities installed. ==="

      rescue:
        - debug:
            msg: "Common utilities install block failed"

      always:
        - import_tasks: log_task_status.yml
          vars:
            task_result: "{{ ansible_failed_task | default({}) }}"
            task_name: "Common Utilities Install"

## ----------------------------
## 11️⃣ Enable Core Services
## ----------------------------
- name: Ensure core services
  hosts: all
  become: yes
  tags:
    - services
  tasks:
    - name: Ensure SELinux enforcing
      block:
        - name: Set SELinux to enforcing
          selinux:
            policy: targeted
            state: enforcing
          register: selinux_result
      rescue:
        - set_fact:
            selinux_result: { failed: true }
      always:
        - import_tasks: log_task_status.yml
          vars:
            task_result: "{{ selinux_result }}"
            task_name: "SELinux Enforcing"

    - name: Ensure Firewall running
      block:
        - name: Start and enable firewalld
          systemd:
            name: firewalld
            state: started
            enabled: yes
          register: firewall_result
      rescue:
        - set_fact:
            firewall_result: { failed: true }
      always:
        - import_tasks: log_task_status.yml
          vars:
            task_result: "{{ firewall_result }}"
            task_name: "Firewall Running"

    - name: Ensure SSHD running
      block:
        - name: Start and enable sshd
          systemd:
            name: sshd
            state: started
            enabled: yes
          register: sshd_result
      rescue:
        - set_fact:
            sshd_result: { failed: true }
      always:
        - import_tasks: log_task_status.yml
          vars:
            task_result: "{{ sshd_result }}"
            task_name: "SSHD Running"

    - name: Ensure NetworkManager running
      block:
        - name: Start and enable NetworkManager
          systemd:
            name: NetworkManager
            state: started
            enabled: yes
          register: nm_result
      rescue:
        - set_fact:
            nm_result: { failed: true }
      always:
        - import_tasks: log_task_status.yml
          vars:
            task_result: "{{ nm_result }}"
            task_name: "NetworkManager Running"


- name: Send email for each host
  hosts: localhost
  gather_facts: no
  vars:
    mail_to: "vigneswaran.murthy@sandisk.com"
    mail_from: "ansible_automation@sandisk.com"
    mail_cc: "PDL-IT-Linux-Support@sandisk.com"
  tasks:
    - name: Read status file for each host
      set_fact:
        host_status_lines: >-
          {{
            (
              lookup('file', playbook_dir ~ '/ansible_output/' ~ item ~ '/status.txt', errors='ignore')
              .splitlines()
            ) if lookup('file', playbook_dir ~ '/ansible_output/' ~ item ~ '/status.txt', errors='ignore') | length > 0
            else ['No tasks executed for this host']
          }}
      loop: "{{ groups['all'] }}"
      register: host_statuses

    - name: Send email for each host
      mail:
        to: "{{ mail_to }}"
        from: "{{ mail_from }}"
        cc: "{{ mail_cc }}"
        subject: "Playbook Task Status Summary - {{ item.item }}"
        subtype: html
        body: |
          <html>
          <body style="font-family: Arial, sans-serif; text-align: center; background-color: #fafafa; padding: 20px;">
            <div style="display: inline-block; text-align: left; background: #fff; padding: 20px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
              <p style="text-align: center; font-size: 16px; font-weight: bold;">
                Playbook Task Status Summary for {{ item.item }}:
              </p>
              <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 600px; margin: 0 auto; text-align: center;">
                <tr style="background-color: #f2f2f2;">
                  <th style="text-align: center;">Task Name</th>
                  <th style="text-align: center;">Status</th>
                </tr>
                {% for line in item.ansible_facts.host_status_lines %}
                  {% set parts = line.split('\t') %}
                  {% set row_color = '#ffffff' if loop.index0 % 2 == 0 else '#f9f9f9' %}
                  {% set status = parts[1] | default('') %}
                  <tr style="background-color: {{ row_color }};">
                    <td style="padding: 6px;">{{ parts[0] | default(line) }}</td>
                    <td style="padding: 6px; text-align: center;">
                      {% if 'Success' in status %}
                        <span style="color: green; font-weight: bold;">✅ Success</span>
                      {% elif 'Failed' in status %}
                        <span style="color: red; font-weight: bold;">❌ Failed</span>
                      {% else %}
                        <span style="color: orange; font-weight: bold;">⚠️ {{ status }}</span>
                      {% endif %}
                    </td>
                  </tr>
                {% endfor %}
              </table>
            </div>
          </body>
          </html>
      loop: "{{ host_statuses.results }}"

